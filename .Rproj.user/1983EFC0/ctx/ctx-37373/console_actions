{
    "type": [
        2,
        2,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        2,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        3,
        3,
        3,
        3,
        3,
        2,
        2,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        2,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        0,
        2,
        0,
        1,
        3,
        2,
        2,
        0,
        1,
        3,
        2,
        2,
        0,
        1,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        2,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        2,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        0,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        2,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        2,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        2
    ],
    "data": [
        "\nR version 4.3.3 (2024-02-29 ucrt) -- \"Angel Food Cake\"\nCopyright (C) 2024 The R Foundation for Statistical Computing\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()'",
        " on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n[Workspace loaded from C:/Users/medai/OneDrive/Bureau/RModels/.RData]\r\n\r\n",
        "> ",
        "#load the data set",
        "> ",
        "dataset <- read.csv(\"MachineLearningChallengeData.csv\")",
        "> ",
        "> ",
        "# Create a new column 'commentlength' with the number of letters (excluding spaces) in each comment",
        "> ",
        "dataset$commentlength <- nchar(gsub(\"\\\\s\", \"\", dataset$comment))",
        "> ",
        "> ",
        "#remove unnecessary columns",
        "> ",
        "dataset <- dataset[, c(2, 3, 5)]",
        "> ",
        "> ",
        "#change columns names",
        "> ",
        "names(dataset) <- c(\"comment\", \"toxic\", \"commentLength\")",
        "> ",
        "> ",
        "#distribution of toxic and non-toxic comments",
        "> ",
        "print(table(dataset$toxic))",
        "\n   0    1 \n9730 4270 \n",
        "> ",
        "> ",
        "# tempray data for testing",
        "> ",
        "data <- dataset",
        "> ",
        "> ",
        "# Remove URLs starting with \"https://\"",
        "> ",
        "data$comment <- gsub(\"https?://[^\\\\s]+\", \"\", data$comment)",
        "> ",
        "> ",
        "# Remove URLs starting with \"http://\"",
        "> ",
        "data$comment <- gsub(\"http?://[^\\\\s]+\", \"\", data$comment)",
        "> ",
        "> ",
        "# Remove punctuation marks",
        "> ",
        "data$comment <- gsub(\"[[:punct:]]\", \"\", data$comment)",
        "> ",
        "> ",
        "# Remove extra white spaces (reducing multiple consecutive spaces to a single space)",
        "> ",
        "data$comment <- gsub(\"\\\\s{2,}\", \" \", data$comment)",
        "> ",
        "> ",
        "# Convert all characters in the 'comment' column to lowercase",
        "> ",
        "data$comment <- tolower(data$comment) ",
        "> ",
        "> ",
        "# Remove names of days",
        "> ",
        "data$comment <- gsub(\"\\\\b(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\\\\b\", \"\", data$comment, ignore.case = TRUE)",
        "> ",
        "> ",
        "# Remove names of months",
        "> ",
        "data$comment <- gsub(\"\\\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\\\b\", \"\", data$comment, ignore.case = TRUE)",
        "> ",
        "> ",
        "# Load the tm package",
        "> ",
        "library(\"tm\")",
        "> ",
        "> ",
        "# Remove English stopwords",
        "> ",
        "data$comment <- removeWords(data$comment, stopwords(\"en\"))",
        "> ",
        "# Remove extra whitespaces (reducing multiple consecutive spaces to a single space)",
        "> ",
        "data$comment <- gsub(\"\\\\s{2,}\", \" \", data$comment)",
        "> ",
        "> ",
        "# Remove digits (numeric characters)",
        "> ",
        "data$comment <- gsub(\"[0-9]\", \"\", data$comment)",
        "> ",
        "#remove words with only one letter",
        "> ",
        "data$comment <- gsub(\"\\\\b\\\\w{1}\\\\b\", \"\", data$comment)",
        "> ",
        "> ",
        "# Sample text data",
        "> ",
        "text_data <- c(\"cats are running\", \"the runners are fast\", \"running\")",
        "> ",
        "# Apply stemming using the textstem package",
        "> ",
        "stemmed_text <- stem_words(text_data)",
        "Error in stem_words(text_data) : could not find function \"stem_words\"\n",
        "> ",
        "# Print the stemmed text",
        "> ",
        "print(stemmed_text)",
        "Error: object 'stemmed_text' not found\n",
        "> ",
        "install.packages(\"textstem\")",
        "WARNING: Rtools is required to build R packages but is not currently installed. Please download and install the appropriate version of Rtools before proceeding:\n\nhttps://cran.rstudio.com/bin/windows/Rtools/\nInstalling package into ‘C:/Users/medai/AppData/Local/R/win-library/4.3’\n(as ‘lib’ is unspecified)\nalso installing the dependencies ‘sylly.en’, ‘sylly’, ‘english’, ‘mgsub’, ‘qdapRegex’, ‘koRpus.lang.en’, ‘hunspell’, ‘koRpus’, ‘lexicon’, ‘textclean’\n\ntrying ",
        "URL 'https://cran.rstudio.com/bin/windows/contrib/4.3/sylly.en_0.1-3.zip'\nContent type 'application/zip' length 277207 bytes (270 KB)\ndownloaded 270 KB\n\ntrying URL 'https://cran.rstudio.com/bin/windows/contrib/4.3/sylly_0.1-6.zip'\nContent type 'application/zip' length 269352 bytes (263 KB)\ndownloaded 263 KB\n\ntrying URL 'https://cran.rstudio.com/bin/windows/contrib/4.3/english_1.2-6.zip'\nContent type 'application/zip' length 131034 bytes (127 KB)\ndownloaded 127 KB\n\ntrying URL 'https://cran.rstudio.com/bin/wi",
        "ndows/contrib/4.3/mgsub_1.7.3.zip'\nContent type 'application/zip' length 46415 bytes (45 KB)\ndownloaded 45 KB\n\ntrying URL 'https://cran.rstudio.com/bin/windows/contrib/4.3/qdapRegex_0.7.8.zip'\nContent type 'application/zip' length 393368 bytes (384 KB)\ndownloaded 384 KB\n\ntrying URL 'https://cran.rstudio.com/bin/windows/contrib/4.3/koRpus.lang.en_0.1-4.zip'\nContent type 'application/zip' length 22875 bytes (22 KB)\ndownloaded 22 KB\n\ntrying URL 'https://cran.rstudio.com/bin/windows/contrib/4.3/hunspell_3.0.3.z",
        "ip'\nContent type 'application/zip' length 1507300 bytes (1.4 MB)\ndownloaded 1.4 MB\n\ntrying URL 'https://cran.rstudio.com/bin/windows/contrib/4.3/koRpus_0.13-8.zip'\nContent type 'application/zip' length 1493250 bytes (1.4 MB)\ndownloaded 1.4 MB\n\ntrying URL 'https://cran.rstudio.com/bin/windows/contrib/4.3/lexicon_1.2.1.zip'\nContent type 'application/zip' length 3252127 bytes (3.1 MB)\ndownloaded 3.1 MB\n\ntrying URL 'https://cran.rstudio.com/bin/windows/contrib/4.3/textclean_0.9.3.zip'\nContent type 'application/",
        "zip' length 1276571 bytes (1.2 MB)\ndownloaded 1.2 MB\n\ntrying URL 'https://cran.rstudio.com/bin/windows/contrib/4.3/textstem_0.1.4.zip'\nContent type 'application/zip' length 128601 bytes (125 KB)\ndownloaded 125 KB\n\n",
        "package ‘sylly.en’ successfully unpacked and MD5 sums checked\npackage ‘sylly’ successfully unpacked and MD5 sums checked\npackage ‘english’ successfully unpacked and MD5 sums checked\npackage ‘mgsub’ successfully unpacked and MD5 sums checked\npackage ‘qdapRegex’ successfully unpacked and MD5 sums checked\npackage ‘koRpus.lang.en’ successfully unpacked and MD5 sums checked\npackage ‘hunspell’ successfully unpacked and MD5 sums checked\npackage ‘koRpus’ successfully unpacked and MD5",
        " sums checked\npackage ‘lexicon’ successfully unpacked and MD5 sums checked\npackage ‘textclean’ successfully unpacked and MD5 sums checked\npackage ‘textstem’ successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n\tC:\\Users\\medai\\AppData\\Local\\Temp\\RtmpMDltaU\\downloaded_packages\n",
        "> ",
        "library(textstem)",
        "Loading required package: koRpus.lang.en\nLoading required package: koRpus\nLoading required package: sylly\nFor information on available language packages for 'koRpus', run\n\n  available.koRpus.lang()\n\nand see ?install.koRpus.lang()\n\n\nAttaching package: ‘koRpus’\n\nThe following object is masked from ‘package:tm’:\n\n    readTagged\n\n",
        "> ",
        "# Sample text data",
        "> ",
        "text_data <- c(\"cats are running\", \"the runners are fast\", \"running\")",
        "> ",
        "# Apply stemming using the textstem package",
        "> ",
        "stemmed_text <- stem_words(text_data)",
        "> ",
        "# Print the stemmed text",
        "> ",
        "print(stemmed_text)",
        "[1] \"cats are run\"         \"the runners are fast\" \"run\"                 \n",
        "> ",
        "library(textstem)",
        "> ",
        "> ",
        "# Apply stemming using the textstem package",
        "> ",
        "data$commen t<- stem_words(data$comment)",
        "Error: unexpected symbol in \"data$commen t\"\n",
        "> ",
        "library(textstem)",
        "> ",
        "> ",
        "# Apply stemming using the textstem package",
        "> ",
        "data$comment <- stem_words(data$comment)",
        "> ",
        "View(data)",
        "> ",
        "library(textstem)",
        "> ",
        "> ",
        "# Apply stemming using the textstem package",
        "> ",
        "data$comment <- stem_words(data$comment)",
        "> ",
        "library(textstem)",
        "> ",
        "> ",
        "# Create a Corpus from the text column",
        "> ",
        "corpus <- Corpus(VectorSource(data$comment))",
        "> ",
        "> ",
        "> ",
        "library(textstem)",
        "> ",
        "> ",
        "# Create a Corpus from the text column",
        "> ",
        "corpus <- Corpus(VectorSource(data$comment))",
        "> ",
        "> ",
        "corpus[[1]]",
        "<<PlainTextDocument>>\nMetadata:  7\nContent:  chars: 121\n",
        "> ",
        "library(textstem)",
        "> ",
        "> ",
        "# Create a Corpus from the text column",
        "> ",
        "corpus <- Corpus(VectorSource(data$comment))",
        "> ",
        "> ",
        "print(corpus[[1]])",
        "<<PlainTextDocument>>\nMetadata:  7\nContent:  chars: 121\n",
        "> ",
        "install.packages(\"SnowballC\")",
        "Error in install.packages : Updating loaded packages\n",
        "> ",
        "library(SnowballC)",
        "> ",
        "> ",
        "\nRestarting R session...\n\n",
        "> ",
        "install.packages(\"SnowballC\")",
        "WARNING: Rtools is required to build R packages but is not currently installed. Please download and install the appropriate version of Rtools before proceeding:\n\nhttps://cran.rstudio.com/bin/windows/Rtools/\nInstalling package into ‘C:/Users/medai/AppData/Local/R/win-library/4.3’\n(as ‘lib’ is unspecified)\ntrying URL 'https://cran.rstudio.com/bin/windows/contrib/4.3/SnowballC_0.7.1.zip'\nContent type 'application/zip' length 364431 bytes (355 KB)\ndownloaded 355 KB\n\n",
        "package ‘SnowballC’ successfully unpacked and MD5 sums checked\nWarning in install.packages :\n  cannot remove prior installation of package ‘SnowballC’\nWarning in install.packages :\n  problem copying C:\\Users\\medai\\AppData\\Local\\R\\win-library\\4.3\\00LOCK\\SnowballC\\libs\\x64\\SnowballC.dll to C:\\Users\\medai\\AppData\\Local\\R\\win-library\\4.3\\SnowballC\\libs\\x64\\SnowballC.dll: Permission denied\nWarning in install.packages :\n  restored ‘SnowballC’\n\nThe downloaded binary packages are in\n\tC:\\Users\\medai\\AppD",
        "ata\\Local\\Temp\\Rtmp6NwpR7\\downloaded_packages\n",
        "> ",
        "install.packages(\"SnowballC\")",
        "WARNING: Rtools is required to build R packages but is not currently installed. Please download and install the appropriate version of Rtools before proceeding:\n\nhttps://cran.rstudio.com/bin/windows/Rtools/\ntrying URL 'https://cran.rstudio.com/bin/windows/contrib/4.3/SnowballC_0.7.1.zip'\nContent type 'application/zip' length 364431 bytes (355 KB)\ndownloaded 355 KB\n\n",
        "package ‘SnowballC’ successfully unpacked and MD5 sums checked\nWarning in install.packages :\n  cannot remove prior installation of package ‘SnowballC’\nWarning in install.packages :\n  problem copying C:\\Users\\medai\\AppData\\Local\\R\\win-library\\4.3\\00LOCK\\SnowballC\\libs\\x64\\SnowballC.dll to C:\\Users\\medai\\AppData\\Local\\R\\win-library\\4.3\\SnowballC\\libs\\x64\\SnowballC.dll: Permission denied\nWarning in install.packages :\n  restored ‘SnowballC’\n\nThe downloaded binary packages are in\n\tC:\\Users\\medai\\AppD",
        "ata\\Local\\Temp\\Rtmp6NwpR7\\downloaded_packages\n",
        "> ",
        "library(SnowballC)",
        "> ",
        "> ",
        "> ",
        "View(data)",
        "> ",
        "#install.packages(\"SnowballC\")",
        "> ",
        "library(SnowballC)",
        "> ",
        "> ",
        "# Apply stemming using the SnowballC package",
        "> ",
        "stemmed_text <- sapply(data$comment, wordStem)",
        "> ",
        "> ",
        "# Replace the original text column with the stemmed text",
        "> ",
        "data$comment <- stemmed_text",
        "> ",
        "> ",
        "> ",
        "View(data)",
        "> ",
        "#load the data set",
        "> ",
        "dataset <- read.csv(\"MachineLearningChallengeData.csv\")",
        "> ",
        "> ",
        "# Create a new column 'commentlength' with the number of letters (excluding spaces) in each comment",
        "> ",
        "dataset$commentlength <- nchar(gsub(\"\\\\s\", \"\", dataset$comment))",
        "> ",
        "> ",
        "#remove unnecessary columns",
        "> ",
        "dataset <- dataset[, c(2, 3, 5)]",
        "> ",
        "> ",
        "#change columns names",
        "> ",
        "names(dataset) <- c(\"comment\", \"toxic\", \"commentLength\")",
        "> ",
        "> ",
        "#distribution of toxic and non-toxic comments",
        "> ",
        "print(table(dataset$toxic))",
        "\n   0    1 \n9730 4270 \n",
        "> ",
        "> ",
        "# tempray data for testing",
        "> ",
        "data <- dataset",
        "> ",
        "> ",
        "# Remove URLs starting with \"https://\"",
        "> ",
        "data$comment <- gsub(\"https?://[^\\\\s]+\", \"\", data$comment)",
        "> ",
        "> ",
        "# Remove URLs starting with \"http://\"",
        "> ",
        "data$comment <- gsub(\"http?://[^\\\\s]+\", \"\", data$comment)",
        "> ",
        "> ",
        "# Remove punctuation marks",
        "> ",
        "data$comment <- gsub(\"[[:punct:]]\", \"\", data$comment)",
        "> ",
        "> ",
        "# Remove extra white spaces (reducing multiple consecutive spaces to a single space)",
        "> ",
        "data$comment <- gsub(\"\\\\s{2,}\", \" \", data$comment)",
        "> ",
        "> ",
        "# Convert all characters in the 'comment' column to lowercase",
        "> ",
        "data$comment <- tolower(data$comment) ",
        "> ",
        "> ",
        "# Remove names of days",
        "> ",
        "data$comment <- gsub(\"\\\\b(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\\\\b\", \"\", data$comment, ignore.case = TRUE)",
        "> ",
        "> ",
        "# Remove names of months",
        "> ",
        "data$comment <- gsub(\"\\\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\\\b\", \"\", data$comment, ignore.case = TRUE)",
        "> ",
        "> ",
        "# Load the tm package",
        "> ",
        "library(\"tm\")",
        "> ",
        "> ",
        "# Remove English stopwords",
        "> ",
        "data$comment <- removeWords(data$comment, stopwords(\"en\"))",
        "> ",
        "# Remove extra whitespaces (reducing multiple consecutive spaces to a single space)",
        "> ",
        "data$comment <- gsub(\"\\\\s{2,}\", \" \", data$comment)",
        "> ",
        "> ",
        "# Remove digits (numeric characters)",
        "> ",
        "data$comment <- gsub(\"[0-9]\", \"\", data$comment)",
        "> ",
        "#remove words with only one letter",
        "> ",
        "data$comment <- gsub(\"\\\\b\\\\w{1}\\\\b\", \"\", data$comment)",
        "> ",
        "> ",
        "#install.packages(\"SnowballC\")",
        "> ",
        "library(SnowballC)",
        "> ",
        "library(tokenizers)",
        "> ",
        "> ",
        "# Tokenize the text column using the tokenizers package",
        "> ",
        "tokenized_text <- tokenize_words(data$comment)",
        "> ",
        "> ",
        "# Stem each token using the SnowballC package",
        "> ",
        "stemmed_tokens <- lapply(tokenized_text, function(tokens) {",
        "+ ",
        "  wordStem(tokens, language = \"en\")  # \"en\" indicates English language",
        "+ ",
        "})",
        "> ",
        "> ",
        "# Combine stemmed tokens back into text",
        "> ",
        "stemmed_text <- sapply(stemmed_tokens, paste, collapse = \" \")",
        "> ",
        "> ",
        "# Replace the original text column with the stemmed text",
        "> ",
        "data$text_column <- stemmed_text",
        "> ",
        "> ",
        "> ",
        "> ",
        "View(data)",
        "> ",
        "#load the data set",
        "> ",
        "dataset <- read.csv(\"MachineLearningChallengeData.csv\")",
        "> ",
        "> ",
        "# Create a new column 'commentlength' with the number of letters (excluding spaces) in each comment",
        "> ",
        "dataset$commentlength <- nchar(gsub(\"\\\\s\", \"\", dataset$comment))",
        "> ",
        "> ",
        "#remove unnecessary columns",
        "> ",
        "dataset <- dataset[, c(2, 3, 5)]",
        "> ",
        "> ",
        "#change columns names",
        "> ",
        "names(dataset) <- c(\"comment\", \"toxic\", \"commentLength\")",
        "> ",
        "> ",
        "#distribution of toxic and non-toxic comments",
        "> ",
        "print(table(dataset$toxic))",
        "\n   0    1 \n9730 4270 \n",
        "> ",
        "> ",
        "# tempray data for testing",
        "> ",
        "data <- dataset",
        "> ",
        "> ",
        "# Remove URLs starting with \"https://\"",
        "> ",
        "data$comment <- gsub(\"https?://[^\\\\s]+\", \"\", data$comment)",
        "> ",
        "> ",
        "# Remove URLs starting with \"http://\"",
        "> ",
        "data$comment <- gsub(\"http?://[^\\\\s]+\", \"\", data$comment)",
        "> ",
        "> ",
        "# Remove punctuation marks",
        "> ",
        "data$comment <- gsub(\"[[:punct:]]\", \"\", data$comment)",
        "> ",
        "> ",
        "# Remove extra white spaces (reducing multiple consecutive spaces to a single space)",
        "> ",
        "data$comment <- gsub(\"\\\\s{2,}\", \" \", data$comment)",
        "> ",
        "> ",
        "# Convert all characters in the 'comment' column to lowercase",
        "> ",
        "data$comment <- tolower(data$comment) ",
        "> ",
        "> ",
        "# Remove names of days",
        "> ",
        "data$comment <- gsub(\"\\\\b(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\\\\b\", \"\", data$comment, ignore.case = TRUE)",
        "> ",
        "> ",
        "# Remove names of months",
        "> ",
        "data$comment <- gsub(\"\\\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\\\b\", \"\", data$comment, ignore.case = TRUE)",
        "> ",
        "> ",
        "# Load the tm package",
        "> ",
        "library(\"tm\")",
        "> ",
        "> ",
        "# Remove English stopwords",
        "> ",
        "data$comment <- removeWords(data$comment, stopwords(\"en\"))",
        "> ",
        "# Remove extra whitespaces (reducing multiple consecutive spaces to a single space)",
        "> ",
        "data$comment <- gsub(\"\\\\s{2,}\", \" \", data$comment)",
        "> ",
        "> ",
        "# Remove digits (numeric characters)",
        "> ",
        "data$comment <- gsub(\"[0-9]\", \"\", data$comment)",
        "> ",
        "#remove words with only one letter",
        "> ",
        "data$comment <- gsub(\"\\\\b\\\\w{1}\\\\b\", \"\", data$comment)",
        "> ",
        "> ",
        "#install.packages(\"SnowballC\")",
        "> ",
        "library(SnowballC)",
        "> ",
        "library(tokenizers)",
        "> ",
        "> ",
        "# Tokenize the text column using the tokenizers package",
        "> ",
        "tokenized_text <- tokenize_words(data$comment)",
        "> ",
        "> ",
        "# Stem each token using the SnowballC package",
        "> ",
        "stemmed_tokens <- lapply(tokenized_text, function(tokens) {",
        "+ ",
        "  wordStem(tokens, language = \"en\")  # \"en\" indicates English language",
        "+ ",
        "})",
        "> ",
        "> ",
        "# Combine stemmed tokens back into text",
        "> ",
        "stemmed_text <- sapply(stemmed_tokens, paste, collapse = \" \")",
        "> ",
        "> ",
        "# Replace the original text column with the stemmed text",
        "> ",
        "data$comment <- stemmed_text",
        "> ",
        "> ",
        "> ",
        "> ",
        "View(data)",
        "\nQuitting from lines 239-245 [unnamed-chunk-18] (ToxicModel.Rmd)\n",
        "> ",
        "View(data)",
        "> ",
        "# Calculate average comment length for toxic and non-toxic comments",
        "> ",
        "avg_length_toxic <- mean(data$commentLength[data$toxic == 1])",
        "> ",
        "avg_length_non_toxic <- mean(data$commentLength[data$toxic == 0])",
        "> ",
        "> ",
        "# Print the average lengths",
        "> ",
        "cat(\"Average comment length for toxic comments:\", avg_length_toxic, \"\\n\")",
        "Average comment length for toxic comments: 220.3789 \n",
        "> ",
        "cat(\"Average comment length for non-toxic comments:\", avg_length_non_toxic, \"\\n\")",
        "Average comment length for non-toxic comments: 331.4431 \n",
        "> ",
        "# Load required library",
        "> ",
        "library(ggplot2)",
        "> ",
        "> ",
        "# Subset data for toxic and non-toxic comments",
        "> ",
        "toxic_comments <- data$commentLength[data$toxic == 1]",
        "> ",
        "non_toxic_comments <- data$commentLength[data$toxic == 0]",
        "> ",
        "> ",
        "# Create histograms ",
        "> ",
        "ggplot() +",
        "+ ",
        "  geom_histogram(aes(x = non_toxic_comments), fill = \"blue\", alpha = 0.5, binwidth = 10) +",
        "+ ",
        "  geom_histogram(aes(x = toxic_comments), fill = \"red\", alpha = 0.5, binwidth = 10) +",
        "+ ",
        "  labs(x = \"Comment Length\", y = \"Frequency\", title = \"Distribution of Comment Lengths for Toxic and Non-Toxic Comments\") +",
        "+ ",
        "  scale_x_continuous(breaks = seq(0, 1000, by = 100), limits = c(0, 800)) +  # Adjust x-axis limits",
        "+ ",
        "  scale_y_continuous(breaks = seq(0, 10000, by = 1000)) +",
        "+ ",
        "  theme_minimal()",
        "> ",
        "> ",
        "> ",
        "> ",
        "# Load the syuzhet library",
        "> ",
        "library(syuzhet)",
        "> ",
        "> ",
        "# Get NRC sentiment scores",
        "> ",
        "sentiment <- get_sentiment(data$comment, method = \"nrc\")",
        "> ",
        "> ",
        "# Convert the sentiment scores to a dataframe",
        "> ",
        "sentiment_scores <- as.data.frame(sentiment)",
        "> ",
        "# combined data to setiment_scores",
        "> ",
        "combined_data <- cbind(data, sentiment_scores)",
        "> ",
        "> ",
        "# explore relationship between sentiment and comment toxicity",
        "> ",
        "# Calculate average comment length for toxic and non-toxic comments",
        "> ",
        "avg_sentiment_toxic <- mean(combined_data$sentiment[combined_data$toxic == 1])",
        "> ",
        "avg_sentiment_non_toxic <- mean(combined_data$sentiment[combined_data$toxic == 0])",
        "> ",
        "> ",
        "# Print the average lengths",
        "> ",
        "cat(\"Average comment sentiment for toxic comments:\", avg_sentiment_toxic, \"\\n\")",
        "Average comment sentiment for toxic comments: -0.6590164 \n",
        "> ",
        "cat(\"Average comment sentiment for non-toxic comments:\", avg_sentiment_non_toxic, \"\\n\")",
        "Average comment sentiment for non-toxic comments: 0.7513875 \n",
        "> ",
        "View(combined_data)",
        "\nQuitting from lines 239-245 [unnamed-chunk-18] (ToxicModel.Rmd)\n",
        "> ",
        "# Create dfm with tf-idf weighting directly from tokens",
        "> ",
        "#dfm_tfidf <- dfm(datatokens, tfidf = TRUE)",
        "> ",
        "dfm_tfidf <- dfm(combined_data$comment, weighting = \"tfidf\")",
        "Error in dfm(combined_data$comment, weighting = \"tfidf\") : \n  could not find function \"dfm\"\n",
        "> ",
        "?dfm",
        "No documentation for ‘dfm’ in specified packages and libraries:\nyou could try ‘??dfm’\n",
        "> ",
        "??dfm",
        "\nQuitting from lines 239-245 [unnamed-chunk-18] (ToxicModel.Rmd)\n",
        "> ",
        "#load the data set",
        "> ",
        "dataset <- read.csv(\"MachineLearningChallengeData.csv\")",
        "> ",
        "dataset$commentlength <- nchar(gsub(\"\\\\s\", \"\", dataset$comment))",
        "> ",
        "#remove unnecessary columns",
        "> ",
        "dataset <- dataset[, c(2, 3, 5)]",
        "> ",
        "#change columns names",
        "> ",
        "names(dataset) <- c(\"comment\", \"toxic\", \"commentLength\")",
        "> ",
        "#distribution of toxic and non-toxic comments",
        "> ",
        "print(table(dataset$toxic))",
        "\n   0    1 \n9730 4270 \n",
        "> ",
        "# tempray data for testing",
        "> ",
        "data <- dataset",
        "> ",
        "train_index <- createDataPartition(data$toxic, p = 0.8, list = FALSE)",
        "Error in createDataPartition(data$toxic, p = 0.8, list = FALSE) : \n  could not find function \"createDataPartition\"\n",
        "> ",
        "library(tm)",
        "> ",
        "library(SnowballC)",
        "> ",
        "library(caret)",
        "Loading required package: lattice\n",
        "> ",
        "train_index <- createDataPartition(data$toxic, p = 0.8, list = FALSE)",
        "> ",
        "train_data <- data[train_index, ]",
        "> ",
        "test_data <- data[-train_index, ]",
        "> ",
        "# Create a document-term matrix (DTM) using TF-IDF",
        "> ",
        "tfidf_vectorizer <- function(data) {",
        "+ ",
        "  corpus <- Corpus(VectorSource(data))",
        "+ ",
        "  dtm <- DocumentTermMatrix(corpus, control = list(tokenize = wordTokenizer))",
        "+ ",
        "  dtm_tfidf <- weightTfIdf(dtm)",
        "+ ",
        "  return(as.matrix(dtm_tfidf))",
        "+ ",
        "}",
        "> ",
        "# Apply TF-IDF vectorizer on training and testing data",
        "> ",
        "X_train <- tfidf_vectorizer(train_data$comment)",
        "Error in tfidf_vectorizer(train_data$comment) : \n  object 'wordTokenizer' not found\n",
        "> ",
        "# Create a document-term matrix (DTM) using TF-IDF",
        "> ",
        "tfidf_vectorizer <- function(data) {",
        "+ ",
        "  corpus <- Corpus(VectorSource(data))",
        "+ ",
        "  dtm <- DocumentTermMatrix(corpus)",
        "+ ",
        "  dtm_tfidf <- weightTfIdf(dtm)",
        "+ ",
        "  return(as.matrix(dtm_tfidf))",
        "+ ",
        "}",
        "> ",
        "# Apply TF-IDF vectorizer on training and testing data",
        "> ",
        "X_train <- tfidf_vectorizer(train_data$comment)",
        "> ",
        "X_test <- tfidf_vectorizer(test_data$comment)",
        "Warning message:\nIn weightTfIdf(dtm) : empty document(s): 704\n",
        "> ",
        "# Define labels",
        "> ",
        "y_train <- train_data$toxic",
        "> ",
        "y_test <- test_data$toxic",
        "> ",
        "# Train logistic regression model",
        "> ",
        "model <- glm(label ~ ., data = cbind(y_train, X_train), family = binomial(link = \"logit\"))",
        "Error in model.frame.default(formula = label ~ ., data = cbind(y_train,  : \n  'data' must be a data.frame, not a matrix or an array\n",
        "> ",
        "# Combine label and TF-IDF matrix into a data frame",
        "> ",
        "train_df <- data.frame(label = y_train, X_train)",
        "> ",
        "test_df <- data.frame(label = y_test, X_test)",
        "> ",
        "# Train logistic regression model",
        "> ",
        "model <- glm(label ~ ., data = train_df, family = binomial(link = \"logit\"))",
        "Error: cannot allocate vector of size 18.9 Gb\n",
        "> ",
        "library(tm)",
        "> ",
        "library(SnowballC)",
        "> ",
        "library(caret)",
        "> ",
        "> ",
        "> ",
        "> ",
        "#load the data set",
        "> ",
        "dataset <- read.csv(\"MachineLearningChallengeData.csv\")",
        "> ",
        "dataset$commentlength <- nchar(gsub(\"\\\\s\", \"\", dataset$comment))",
        "> ",
        "#remove unnecessary columns",
        "> ",
        "dataset <- dataset[, c(2, 3, 5)]",
        "> ",
        "> ",
        "#change columns names",
        "> ",
        "names(dataset) <- c(\"comment\", \"toxic\", \"commentLength\")",
        "> ",
        "> ",
        "#distribution of toxic and non-toxic comments",
        "> ",
        "print(table(dataset$toxic))",
        "\n   0    1 \n9730 4270 \n",
        "> ",
        "> ",
        "# tempray data for testing",
        "> ",
        "data <- dataset",
        "> ",
        "> ",
        "# Split the data into training and testing sets",
        "> ",
        "set.seed(42)  # for reproducibility",
        "> ",
        "train_index <- createDataPartition(data$toxic, p = 0.8, list = FALSE)",
        "> ",
        "train_data <- data[train_index, ]",
        "> ",
        "test_data <- data[-train_index, ]",
        "> ",
        "> ",
        "# Create a document-term matrix (DTM) using TF-IDF",
        "> ",
        "tfidf_vectorizer <- function(data) {",
        "+ ",
        "  corpus <- Corpus(VectorSource(data))",
        "+ ",
        "  dtm <- DocumentTermMatrix(corpus)",
        "+ ",
        "  dtm_tfidf <- weightTfIdf(dtm)",
        "+ ",
        "  ",
        "+ ",
        "  # Reduce the size of the TF-IDF matrix by keeping only the top 1000 features",
        "+ ",
        "  dtm_tfidf_reduced <- removeSparseTerms(dtm_tfidf, sparse = 0.999)",
        "+ ",
        "  ",
        "+ ",
        "  return(as.matrix(dtm_tfidf_reduced))",
        "+ ",
        "}",
        "> ",
        "> ",
        "# Apply TF-IDF vectorizer on training and testing data",
        "> ",
        "X_train <- tfidf_vectorizer(train_data$comment)",
        "> ",
        "library(tm)",
        "> ",
        "library(SnowballC)",
        "> ",
        "library(caret)",
        "> ",
        "#load the data set",
        "> ",
        "dataset <- read.csv(\"MachineLearningChallengeData.csv\")",
        "> ",
        "dataset$commentlength <- nchar(gsub(\"\\\\s\", \"\", dataset$comment))",
        "> ",
        "#remove unnecessary columns",
        "> ",
        "dataset <- dataset[, c(2, 3, 5)]",
        "> ",
        "#change columns names",
        "> ",
        "names(dataset) <- c(\"comment\", \"toxic\", \"commentLength\")",
        "> ",
        "#distribution of toxic and non-toxic comments",
        "> ",
        "print(table(dataset$toxic))",
        "\n   0    1 \n9730 4270 \n",
        "> ",
        "# tempray data for testing",
        "> ",
        "data <- dataset",
        "> ",
        "View(data)",
        "> ",
        "library(tm)",
        "> ",
        "library(SnowballC)",
        "> ",
        "library(caret)",
        "> ",
        "#load the data set",
        "> ",
        "dataset <- read.csv(\"MachineLearningChallengeData.csv\")",
        "> ",
        "dataset$commentlength <- nchar(gsub(\"\\\\s\", \"\", dataset$comment))",
        "> ",
        "#remove unnecessary columns",
        "> ",
        "dataset <- dataset[, c(2, 3, 5)]",
        "> ",
        "#change columns names",
        "> ",
        "names(dataset) <- c(\"comment\", \"toxic\", \"commentLength\")",
        "> ",
        "#distribution of toxic and non-toxic comments",
        "> ",
        "print(table(dataset$toxic))",
        "\n   0    1 \n9730 4270 \n",
        "> ",
        "# tempray data for testing",
        "> ",
        "data <- dataset",
        "> ",
        "# Remove URLs starting with \"https://\"",
        "> ",
        "data$comment <- gsub(\"https?://[^\\\\s]+\", \"\", data$comment)",
        "> ",
        "# Remove URLs starting with \"http://\"",
        "> ",
        "data$comment <- gsub(\"http?://[^\\\\s]+\", \"\", data$comment)",
        "> ",
        "# Remove punctuation marks",
        "> ",
        "data$comment <- gsub(\"[[:punct:]]\", \"\", data$comment)",
        "> ",
        "# Remove extra white spaces (reducing multiple consecutive spaces to a single space)",
        "> ",
        "data$comment <- gsub(\"\\\\s{2,}\", \" \", data$comment)",
        "> ",
        "# Convert all characters in the 'comment' column to lowercase",
        "> ",
        "data$comment <- tolower(data$comment) ",
        "> ",
        "# Remove names of days",
        "> ",
        "data$comment <- gsub(\"\\\\b(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\\\\b\", \"\", data$comment, ignore.case = TRUE)",
        "> ",
        "# Remove names of months",
        "> ",
        "data$comment <- gsub(\"\\\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\\\b\", \"\", data$comment, ignore.case = TRUE)",
        "> ",
        "# Remove English stopwords",
        "> ",
        "data$comment <- removeWords(data$comment, stopwords(\"en\"))",
        "> ",
        "# Remove extra whitespaces (reducing multiple consecutive spaces to a single space)",
        "> ",
        "data$comment <- gsub(\"\\\\s{2,}\", \" \", data$comment)",
        "> ",
        "# Remove digits (numeric characters)",
        "> ",
        "data$comment <- gsub(\"[0-9]\", \"\", data$comment)",
        "> ",
        "#remove words with only one letter",
        "> ",
        "data$comment <- gsub(\"\\\\b\\\\w{1}\\\\b\", \"\", data$comment)",
        "> ",
        "install.packages(\"tokenizers\")",
        "Error in install.packages : Updating loaded packages\n",
        "> ",
        "install.packages(\"SnowballC\")",
        "Error in install.packages : Updating loaded packages\n",
        "> ",
        "library(SnowballC)",
        "> ",
        "library(tokenizers)",
        "> ",
        "# Tokenize the text column using the tokenizers package",
        "> ",
        "tokenized_text <- tokenize_words(data$comment)",
        "> ",
        "# Stem each token using the SnowballC package",
        "> ",
        "stemmed_tokens <- lapply(tokenized_text, function(tokens) {",
        "+ ",
        "  wordStem(tokens, language = \"en\")  # \"en\" indicates English language",
        "+ ",
        "})",
        "> ",
        "# Combine stemmed tokens back into text",
        "> ",
        "stemmed_text <- sapply(stemmed_tokens, paste, collapse = \" \")",
        "> ",
        "# Replace the original text column with the stemmed text",
        "> ",
        "data$comment <- stemmed_text",
        "> ",
        "install.packages(\"tokenizers\")",
        "WARNING: Rtools is required to build R packages but is not currently installed. Please download and install the appropriate version of Rtools before proceeding:\n\nhttps://cran.rstudio.com/bin/windows/Rtools/\n",
        "Warning in install.packages :\n  package ‘tokenizers’ is in use and will not be installed\n\nRestarting R session...\n\n"
    ]
}