theme_minimal()
##### Comment Sentiment:
# Load the syuzhet library
library(syuzhet)
# Get NRC sentiment scores
sentiment <- get_sentiment(data$comment, method = "nrc")
# Convert the sentiment scores to a dataframe
sentiment_scores <- as.data.frame(sentiment)
# combined data to setiment_scores
data <- cbind(data, sentiment_scores)
# explore relationship between sentiment and comment toxicity
# Calculate average comment length for toxic and non-toxic comments
avg_sentiment_toxic <- mean(data$sentiment[data$toxic == 1])
avg_sentiment_non_toxic <- mean(data$sentiment[data$toxic == 0])
# Print the average lengths
cat("Average comment sentiment for toxic comments:", avg_sentiment_toxic, "\n")
cat("Average comment sentiment for non-toxic comments:", avg_sentiment_non_toxic, "\n")
##### TF-IDF Matrix:
# Create a document-term matrix (DTM) using TF-IDF
tfidf_vectorizer <- function(data) {
corpus <- Corpus(VectorSource(data))
dtm <- DocumentTermMatrix(corpus)
dtm_tfidf <- weightTfIdf(dtm)
# Reduce the size of the TF-IDF matrix by keeping only the top 1000 features
dtm_tfidf_reduced <- removeSparseTerms(dtm_tfidf, sparse = 0.999)
return(as.matrix(dtm_tfidf_reduced))
}
# Apply TF-IDF vectorizer on data
tfidf_data <- tfidf_vectorizer(data$comment)
# Define labels
target <- data$toxic
# Combine label and TF-IDF matrix into a data frame
combined_data <- data.frame(label = target, tfidf_data)
#### 4-Model Building:
# Split the data into training and testing sets
set.seed(42)  # for reproducibility
train_index <- createDataPartition(combined_data$label, p = 0.8, list = FALSE)
train_data <- combined_data[train_index, ]
test_data <- combined_data[-train_index, ]
##### Naive Bayes:
#Train Naive Bayes model
library(e1071)
naive_bayes_model <- naiveBayes(as.factor(label) ~ ., data = train_data)
# Make predictions
naive_bayes_predictions <- predict(naive_bayes_model, newdata = test_data)
# Load required libraries
library(ggplot2)
library(tm)
library(SnowballC)
library(caret)
library(tokenizers)
#load the data set
dataset2 <- read.csv("MachineLearningChallengeData.csv")
# Create a new column 'commentlength' with the number of letters (excluding spaces) in each comment
dataset2$commentlength <- nchar(gsub("\\s", "", dataset2$comment))
#remove unnecessary columns
dataset2 <- dataset2[, c(2, 4, 5)]
#change columns names
names(dataset2) <- c("comment", "insult", "commentLength")
#distribution of toxic and non-toxic comments
print(table(dataset2$insult))
# temporary data for testing
data2 <- dataset2
# Find missing values
print(which(is.na(data2$comment)))
print(which(is.na(data2$insult)))
#there is no missing valuesin both columns
# Remove URLs starting with "https://"
data2$comment <- gsub("https?://[^\\s]+", "", data2$comment)
# Remove URLs starting with "http://"
data2$comment <- gsub("http?://[^\\s]+", "", data2$comment)
# Remove punctuation marks
data2$comment <- gsub("[[:punct:]]", "", data2$comment)
# Remove extra white spaces (reducing multiple consecutive spaces to a single space)
data2$comment <- gsub("\\s{2,}", " ", data2$comment)
# Convert all characters in the 'comment' column to lowercase
data2$comment <- tolower(data2$comment)
# Remove names of days
data2$comment <- gsub("\\b(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\\b", "", data2$comment, ignore.case = TRUE)
# Remove names of months
data2$comment <- gsub("\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\b", "", data2$comment, ignore.case = TRUE)
# Remove English stopwords
data2$comment <- removeWords(data2$comment, stopwords("en"))
# Remove extra whitespaces (reducing multiple consecutive spaces to a single space)
data2$comment <- gsub("\\s{2,}", " ", data2$comment)
# Remove digits (numeric characters)
data2$comment <- gsub("[0-9]", "", data2$comment)
#remove words with only one letter
data2$comment <- gsub("\\b\\w{1}\\b", "", data2$comment)
# Tokenize the text column using the tokenizers package
tokenized_text2 <- tokenize_words(data2$comment)
# Stem each token using the SnowballC package
stemmed_tokens2 <- lapply(tokenized_text2, function(tokens) {
wordStem(tokens, language = "en")  # "en" indicates English language
})
# Combine stemmed tokens back into text
stemmed_text2 <- sapply(stemmed_tokens2, paste, collapse = " ")
# Replace the original text column with the stemmed text
data2$comment <- stemmed_text2
# Calculate average comment length for insult and non-insult comments
avg_length_insult <- mean(data2$commentLength[data2$insult == 1])
avg_length_non_insult <- mean(data2$commentLength[data2$insult == 0])
# Print the average lengths
cat("Average comment length for insult comments:", avg_length_insult, "\n")
cat("Average comment length for non-insult comments:", avg_length_non_insult, "\n")
# Subset data for insult and non-insult comments
insult_comments <- data2$commentLength[data2$insult == 1]
non_insult_comments <- data2$commentLength[data2$insult == 0]
# Create histograms
ggplot() +
geom_histogram(aes(x = non_insult_comments), fill = "yellow", alpha = 0.5, binwidth = 10) +
geom_histogram(aes(x = insult_comments), fill = "green", alpha = 0.5, binwidth = 10) +
labs(x = "Comment Length", y = "Frequency", title = "Distribution of Comment Lengths for Insult and Non-Insult Comments") +
scale_x_continuous(breaks = seq(0, 1000, by = 100), limits = c(0, 800)) +  # Adjust x-axis limits
scale_y_continuous(breaks = seq(0, 10000, by = 1000)) +
theme_minimal()
# Create a document-term matrix (Bag of Words)
dtm2 <- DocumentTermMatrix(data2$comment)
# Convert TDM to a matrix
dtm_matrix <- as.matrix(dtm2)
# Calculate term frequencies
term_freq <- colSums(dtm_matrix)
# Select the top 1200 most frequent features
top_features <- names(sort(term_freq, decreasing = TRUE)[1:1200])
# Filter DTM to include only top features
filtered_dtm_matrix <- dtm_matrix[, top_features]
# Combine label and bof matrix into a data frame
combined_data2 <- data.frame(label = data2$insult, commentLength = data2$commentLength, filtered_dtm_matrix)
# Split the data into training and testing sets
set.seed(42)  # for reproducibility
train_index2 <- createDataPartition(combined_data2$label, p = 0.8, list = FALSE)
train_data2 <- combined_data2[train_index2, ]
test_data2 <- combined_data2[-train_index2, ]
#Train Naive Bayes model
library(e1071)
naive_bayes_model2 <- naiveBayes(as.factor(label) ~ ., data = train_data2)
# Make predictions
naive_bayes2_predictions <- predict(naive_bayes_model2, newdata = test_data2)
# Calculate confusion matrix
naive_bayes2_confusion_matrix <- confusionMatrix(naive_bayes2_predictions, factor(test_data2$label, levels = c(0, 1)))
# Print confusion matrix
print(naive_bayes2_confusion_matrix)
# Extract metrics for Naive Bayes model
naive_bayes2_precision <- naive_bayes2_confusion_matrix$byClass["Pos Pred Value"]
naive_bayes2_recall <- naive_bayes2_confusion_matrix$byClass["Sensitivity"]
naive_bayes2_f1_score <- naive_bayes2_confusion_matrix$byClass["F1"]
naive_bayes2_accuracy <- naive_bayes2_confusion_matrix$overall["Accuracy"]
naive_bayes2_specificity <- naive_bayes2_confusion_matrix$byClass["Specificity"]
naive_bayes2_balanced_accuracy <- naive_bayes2_confusion_matrix$byClass["Balanced Accuracy"]
naive_bayes2_kappa <- naive_bayes2_confusion_matrix$overall["Kappa"]
# Print metrics for Naive Bayes model
cat("Naive Bayes Metrics:\n")
cat("Precision:", naive_bayes2_precision, "\n")
cat("Recall:", naive_bayes2_recall, "\n")
cat("F1 Score:", naive_bayes2_f1_score, "\n")
cat("Accuracy:", naive_bayes2_accuracy, "\n")
cat("Specificity:", naive_bayes2_specificity, "\n")
cat("Balanced Accuracy:", naive_bayes2_balanced_accuracy, "\n")
cat("Kappa:", naive_bayes2_kappa, "\n")
# Load pROC package
library(pROC)
# Calculate ROC curve
roc_curve <- roc(test_data2$label, naive_bayes2_predictions)
naive_bayes3_predictions <- predict(naive_bayes2_model, newdata = test_data2, type = "prob")
#Train Naive Bayes model
library(e1071)
naive_bayes_model2 <- naiveBayes(as.factor(label) ~ ., data = train_data2)
# Make predictions
naive_bayes2_predictions <- predict(naive_bayes_model2, newdata = test_data2)
naive_bayes3_predictions <- predict(naive_bayes2_model, newdata = test_data2, type = "prob")
#Train Naive Bayes model
library(e1071)
naive_bayes_model2 <- naiveBayes(as.factor(label) ~ ., data = train_data2)
# Make predictions
naive_bayes2_predictions <- predict(naive_bayes_model2, newdata = test_data2)
naive_bayes3_predictions <- predict(naive_bayes_model2, newdata = test_data2, type = "prob")
#Train Naive Bayes model
library(e1071)
naive_bayes_model2 <- naiveBayes(as.factor(label) ~ ., data = train_data2)
# Make predictions
naive_bayes2_predictions <- predict(naive_bayes_model2, newdata = test_data2)
# Predict class probabilities
naive_bayes3_predictions_raw <- predict(naive_bayes_model2, newdata = test_data2, type = "raw")
# Normalize probabilities to obtain probabilities between 0 and 1
naive_bayes3_predictions <- apply(naive_bayes3_predictions_raw, 1, function(x) exp(x) / sum(exp(x)))
# Calculate confusion matrix
naive_bayes2_confusion_matrix <- confusionMatrix(naive_bayes2_predictions, factor(test_data2$label, levels = c(0, 1)))
# Print confusion matrix
print(naive_bayes2_confusion_matrix)
# Extract metrics for Naive Bayes model
naive_bayes2_precision <- naive_bayes2_confusion_matrix$byClass["Pos Pred Value"]
naive_bayes2_recall <- naive_bayes2_confusion_matrix$byClass["Sensitivity"]
naive_bayes2_f1_score <- naive_bayes2_confusion_matrix$byClass["F1"]
naive_bayes2_accuracy <- naive_bayes2_confusion_matrix$overall["Accuracy"]
naive_bayes2_specificity <- naive_bayes2_confusion_matrix$byClass["Specificity"]
naive_bayes2_balanced_accuracy <- naive_bayes2_confusion_matrix$byClass["Balanced Accuracy"]
naive_bayes2_kappa <- naive_bayes2_confusion_matrix$overall["Kappa"]
# Print metrics for Naive Bayes model
cat("Naive Bayes Metrics:\n")
cat("Precision:", naive_bayes2_precision, "\n")
cat("Recall:", naive_bayes2_recall, "\n")
cat("F1 Score:", naive_bayes2_f1_score, "\n")
cat("Accuracy:", naive_bayes2_accuracy, "\n")
cat("Specificity:", naive_bayes2_specificity, "\n")
cat("Balanced Accuracy:", naive_bayes2_balanced_accuracy, "\n")
cat("Kappa:", naive_bayes2_kappa, "\n")
# Load pROC package
library(pROC)
# Calculate ROC curve
roc_curve <- roc(test_data2$label, naive_bayes3_predictions)
# Load pROC package
library(pROC)
# Calculate ROC curve
roc_curve <- roc(test_data2$label, naive_bayes3_predictions_raw)
dim(test_data2)
dim(naive_bayes3_predictions_raw)
dim(test_data2)
dim(naive_bayes3_predictions_raw)
dim(naive_bayes3_predictions_raw)
dim(test_data2)
#Train Naive Bayes model
library(e1071)
naive_bayes_model2 <- naiveBayes(as.factor(label) ~ ., data = train_data2)
# Make predictions
naive_bayes2_predictions <- predict(naive_bayes_model2, newdata = test_data2)
naive_bayes1_predictions <- predict(naive_bayes_model2, newdata = test_data1, type = "raw")[, "1"]
naive_bayes1_predictions <- predict(naive_bayes_model2, newdata = test_data2, type = "raw")[, "1"]
library(pROC)
roc_curve1 <- roc(test_data2$label, naive_bayes1_predictions)
auc_score1 <- auc(roc_curve1)
plot(roc_curve1, col = "blue", main = "ROC Curve for Naive Bayes Model")
# Train Decision Trees model
library(rpart)
decision_trees_model <- rpart(as.factor(label) ~ ., data = train_data2)
# Make predictions
decision_trees_predictions <- predict(decision_trees_model, newdata = test_data2, type = "class")
# Calculate confusion matrix
decision_trees_confusion_matrix <- confusionMatrix(decision_trees_predictions, factor(test_data2$label, levels = c(0, 1)))
# Print confusion matrix
print(decision_trees_confusion_matrix)
# Extract metrics for Decision Trees model
decision_trees_precision <- decision_trees_confusion_matrix$byClass["Pos Pred Value"]
decision_trees_recall <- decision_trees_confusion_matrix$byClass["Sensitivity"]
decision_trees_f1_score <- decision_trees_confusion_matrix$byClass["F1"]
decision_trees_accuracy <- decision_trees_confusion_matrix$overall["Accuracy"]
decision_trees_specificity <- decision_trees_confusion_matrix$byClass["Specificity"]
decision_trees_balanced_accuracy <- decision_trees_confusion_matrix$byClass["Balanced Accuracy"]
decision_trees_kappa <- decision_trees_confusion_matrix$overall["Kappa"]
# Print metrics for Decision Trees model
cat("\nDecision Trees Metrics:\n")
cat("Precision:", decision_trees_precision, "\n")
cat("Recall:", decision_trees_recall, "\n")
cat("F1 Score:", decision_trees_f1_score, "\n")
cat("Accuracy:", decision_trees_accuracy, "\n")
cat("Specificity:", decision_trees_specificity, "\n")
cat("Balanced Accuracy:", decision_trees_balanced_accuracy, "\n")
cat("Kappa:", decision_trees_kappa, "\n")
# Calculate ROC curve and AUC
roc_curve <- roc(test_data2$label, decision_tree_predictions)
# Calculate ROC curve and AUC
roc_curve <- roc(test_data2$label, decision_trees_predictions)
# Generate predicted probabilities
decision_tree_predictions <- predict(decision_trees_model, newdata = test_data2, type = "prob")[, "1"]
# Calculate ROC curve and AUC
roc_curve <- roc(test_data2$label, decision_tree_predictions)
auc_score <- auc(roc_curve)
plot(roc_curve, main = "ROC Curve for Decision Tree Classifier")
plot(roc_curve1, col = "blue", main = "ROC Curves for Naive Bayes and Decision Trees")
plot(roc_curve, col = "red", add = TRUE)
plot(roc_curve1, col = "blue", main = "ROC Curves for Naive Bayes and Decision Trees")
plot(roc_curve, col = "red", add = TRUE)
# Add legend
legend("bottomright", legend = c("Naive Bayes", "Decision Trees"), col = c("blue", "red"), lty = 1, cex = 0.8)
?roc
# Calculate ROC curve and AUC
roc_curve <- roc(test_data2$label, decision_tree_predictions, smoth = TRUE)
auc_score <- auc(roc_curve)
plot(roc_curve1, col = "blue", main = "ROC Curves for Naive Bayes and Decision Trees")
plot(roc_curve, col = "red", add = TRUE)
plot(roc_curve1, col = "blue", main = "ROC Curves for Naive Bayes and Decision Trees")
plot(roc_curve, col = "red", add = TRUE)
library(ggplot2)
plot(roc_curve1, col = "blue", main = "ROC Curves for Naive Bayes and Decision Trees")
plot(roc_curve, col = "red", add = TRUE)
plot(roc_curve, col = "red", add = TRUE)
# Calculate ROC curve and AUC
roc_curve <- roc(test_data2$label, decision_tree_predictions, smooth = TRUE)
auc_score <- auc(roc_curve)
plot(roc_curve1, col = "blue", main = "ROC Curves for Naive Bayes and Decision Trees")
plot(roc_curve, col = "red", add = TRUE)
# Add legend
legend("bottomright", legend = c("Naive Bayes", "Decision Trees"), col = c("blue", "red"), lty = 1, cex = 0.8)
```{r}
# Load required libraries
library(ggplot2)
# Load required libraries
library(ggplot2)
library(tm)
# Create a new column 'commentlength' with the number of letters (excluding spaces) in each comment
dataset2$commentlength <- nchar(gsub("\\s", "", dataset2$comment))
#load the data set
dataset2 <- read.csv("MachineLearningChallengeData.csv")
# Create a new column 'commentlength' with the number of letters (excluding spaces) in each comment
dataset2$commentlength <- nchar(gsub("\\s", "", dataset2$comment))
#remove unnecessary columns
dataset2 <- dataset2[, c(2, 4, 5)]
#change columns names
names(dataset2) <- c("comment", "insult", "commentLength")
#distribution of toxic and non-toxic comments
print(table(dataset2$insult))
# temporary data for testing
data2 <- dataset2
# Find missing values
print(which(is.na(data2$comment)))
print(which(is.na(data2$insult)))
#there is no missing valuesin both columns
# Remove URLs starting with "https://"
data2$comment <- gsub("https?://[^\\s]+", "", data2$comment)
# Remove URLs starting with "http://"
data2$comment <- gsub("http?://[^\\s]+", "", data2$comment)
# Remove punctuation marks
data2$comment <- gsub("[[:punct:]]", "", data2$comment)
# Remove extra white spaces (reducing multiple consecutive spaces to a single space)
data2$comment <- gsub("\\s{2,}", " ", data2$comment)
# Convert all characters in the 'comment' column to lowercase
data2$comment <- tolower(data2$comment)
# Remove names of days
data2$comment <- gsub("\\b(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\\b", "", data2$comment, ignore.case = TRUE)
# Remove names of months
data2$comment <- gsub("\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\b", "", data2$comment, ignore.case = TRUE)
# Remove English stopwords
data2$comment <- removeWords(data2$comment, stopwords("en"))
# Remove extra whitespaces (reducing multiple consecutive spaces to a single space)
data2$comment <- gsub("\\s{2,}", " ", data2$comment)
# Remove digits (numeric characters)
data2$comment <- gsub("[0-9]", "", data2$comment)
#remove words with only one letter
data2$comment <- gsub("\\b\\w{1}\\b", "", data2$comment)
# Tokenize the text column using the tokenizers package
tokenized_text2 <- tokenize_words(data2$comment)
# Tokenize the text column using the tokenizers package
tokenized_text2 <- tokenize_words(data2$comment)
library(tokenizers)
# Tokenize the text column using the tokenizers package
tokenized_text2 <- tokenize_words(data2$comment)
# Stem each token using the SnowballC package
stemmed_tokens2 <- lapply(tokenized_text2, function(tokens) {
wordStem(tokens, language = "en")  # "en" indicates English language
})
library(ggplot2)
library(tm)
library(SnowballC)
library(caret)
library(tokenizers)
stemmed_tokens2 <- lapply(tokenized_text2, function(tokens) {
wordStem(tokens, language = "en")  # "en" indicates English language
})
# Combine stemmed tokens back into text
stemmed_text2 <- sapply(stemmed_tokens2, paste, collapse = " ")
# Replace the original text column with the stemmed text
data2$comment <- stemmed_text2
# Calculate average comment length for insult and non-insult comments
avg_length_insult <- mean(data2$commentLength[data2$insult == 1])
avg_length_non_insult <- mean(data2$commentLength[data2$insult == 0])
# Print the average lengths
cat("Average comment length for insult comments:", avg_length_insult, "\n")
cat("Average comment length for non-insult comments:", avg_length_non_insult, "\n")
# Subset data for insult and non-insult comments
insult_comments <- data2$commentLength[data2$insult == 1]
non_insult_comments <- data2$commentLength[data2$insult == 0]
# Create histograms
ggplot() +
geom_histogram(aes(x = non_insult_comments), fill = "yellow", alpha = 0.5, binwidth = 10) +
geom_histogram(aes(x = insult_comments), fill = "green", alpha = 0.5, binwidth = 10) +
labs(x = "Comment Length", y = "Frequency", title = "Distribution of Comment Lengths for Insult and Non-Insult Comments") +
scale_x_continuous(breaks = seq(0, 1000, by = 100), limits = c(0, 800)) +  # Adjust x-axis limits
scale_y_continuous(breaks = seq(0, 10000, by = 1000)) +
theme_minimal()
# Create a document-term matrix (Bag of Words)
dtm2 <- DocumentTermMatrix(data2$comment)
# Convert TDM to a matrix
dtm_matrix <- as.matrix(dtm2)
# Calculate term frequencies
term_freq <- colSums(dtm_matrix)
# Select the top 1200 most frequent features
top_features <- names(sort(term_freq, decreasing = TRUE)[1:1200])
# Filter DTM to include only top features
filtered_dtm_matrix <- dtm_matrix[, top_features]
# Combine label and bof matrix into a data frame
combined_data2 <- data.frame(label = data2$insult, commentLength = data2$commentLength, filtered_dtm_matrix)
# Split the data into training and testing sets
set.seed(42)  # for reproducibility
train_index2 <- createDataPartition(combined_data2$label, p = 0.8, list = FALSE)
train_data2 <- combined_data2[train_index2, ]
test_data2 <- combined_data2[-train_index2, ]
#Train Naive Bayes model
library(e1071)
naive_bayes_model2 <- naiveBayes(as.factor(label) ~ ., data = train_data2)
# Make predictions
naive_bayes2_predictions <- predict(naive_bayes_model2, newdata = test_data2)
# Calculate confusion matrix
naive_bayes2_confusion_matrix <- confusionMatrix(naive_bayes2_predictions, factor(test_data2$label, levels = c(0, 1)))
# Print confusion matrix
print(naive_bayes2_confusion_matrix)
# Extract metrics for Naive Bayes model
naive_bayes2_precision <- naive_bayes2_confusion_matrix$byClass["Pos Pred Value"]
naive_bayes2_recall <- naive_bayes2_confusion_matrix$byClass["Sensitivity"]
naive_bayes2_f1_score <- naive_bayes2_confusion_matrix$byClass["F1"]
naive_bayes2_accuracy <- naive_bayes2_confusion_matrix$overall["Accuracy"]
naive_bayes2_specificity <- naive_bayes2_confusion_matrix$byClass["Specificity"]
naive_bayes2_balanced_accuracy <- naive_bayes2_confusion_matrix$byClass["Balanced Accuracy"]
naive_bayes2_kappa <- naive_bayes2_confusion_matrix$overall["Kappa"]
# Print metrics for Naive Bayes model
cat("Naive Bayes Metrics:\n")
cat("Precision:", naive_bayes2_precision, "\n")
cat("Recall:", naive_bayes2_recall, "\n")
cat("F1 Score:", naive_bayes2_f1_score, "\n")
cat("Accuracy:", naive_bayes2_accuracy, "\n")
cat("Specificity:", naive_bayes2_specificity, "\n")
cat("Balanced Accuracy:", naive_bayes2_balanced_accuracy, "\n")
cat("Kappa:", naive_bayes2_kappa, "\n")
# Train Decision Trees model
library(rpart)
decision_trees_model <- rpart(as.factor(label) ~ ., data = train_data2)
# Make predictions
decision_trees_predictions <- predict(decision_trees_model, newdata = test_data2, type = "class")
# Calculate confusion matrix
decision_trees_confusion_matrix <- confusionMatrix(decision_trees_predictions, factor(test_data2$label, levels = c(0, 1)))
# Print confusion matrix
print(decision_trees_confusion_matrix)
# Extract metrics for Decision Trees model
decision_trees_precision <- decision_trees_confusion_matrix$byClass["Pos Pred Value"]
decision_trees_recall <- decision_trees_confusion_matrix$byClass["Sensitivity"]
decision_trees_f1_score <- decision_trees_confusion_matrix$byClass["F1"]
decision_trees_accuracy <- decision_trees_confusion_matrix$overall["Accuracy"]
decision_trees_specificity <- decision_trees_confusion_matrix$byClass["Specificity"]
decision_trees_balanced_accuracy <- decision_trees_confusion_matrix$byClass["Balanced Accuracy"]
decision_trees_kappa <- decision_trees_confusion_matrix$overall["Kappa"]
# Print metrics for Decision Trees model
cat("\nDecision Trees Metrics:\n")
cat("Precision:", decision_trees_precision, "\n")
cat("Recall:", decision_trees_recall, "\n")
cat("F1 Score:", decision_trees_f1_score, "\n")
cat("Accuracy:", decision_trees_accuracy, "\n")
cat("Specificity:", decision_trees_specificity, "\n")
cat("Balanced Accuracy:", decision_trees_balanced_accuracy, "\n")
cat("Kappa:", decision_trees_kappa, "\n")
library(pROC)
library(ggplot2)
#naive bayes
naive_bayes1_predictions <- predict(naive_bayes_model2, newdata = test_data2, type = "raw")[, "1"]
# Calculate ROC curve and AUC
roc_curve1 <- roc(test_data2$label, naive_bayes1_predictions)
auc_score1 <- auc(roc_curve1)
#Decision trees
decision_tree_predictions <- predict(decision_trees_model, newdata = test_data2, type = "prob")[, "1"]
# Calculate ROC curve and AUC
roc_curve <- roc(test_data2$label, decision_tree_predictions)
auc_score <- auc(roc_curve)
plot(roc_curve1, col = "blue", main = "ROC Curves for Naive Bayes and Decision Trees")
plot(roc_curve, col = "red", add = TRUE)
# Add legend
legend("bottomright", legend = c("Naive Bayes", "Decision Trees"), col = c("blue", "red"), lty = 1, cex = 0.8)
# Plot ROC curves with customizations
plot(roc_curve, col = "blue", main = "ROC Curves for Naive Bayes and Decision Trees",
xlim = c(0, 1), ylim = c(0, 1), type = "l", lwd = 2, xlab = "False Positive Rate (1 - Specificity)",
ylab = "True Positive Rate (Sensitivity)")
plot(roc_curve1, col = "red", add = TRUE, lwd = 2)
# Add legend with clear colors and labels
legend("bottomright", legend = c("Naive Bayes", "Decision Trees"), col = c("blue", "red"), lty = 1, lwd = 2, cex = 0.8)
# Add grid lines for better readability
grid()
# Add a smooth line through the ROC curve
smooth_ROC_curve <- smooth(roc_curve, method = "spline")
# Define pastel colors
pastel_blue <- "#ADD8E6"  # Light blue
pastel_red <- "#FFC0CB"   # Light pink
# Plot ROC curves with pastel colors
plot(roc_curve, col = pastel_blue, main = "ROC Curves for Naive Bayes and Decision Trees",
xlim = c(0, 1), ylim = c(0, 1), type = "l", lwd = 2, xlab = "False Positive Rate (1 - Specificity)",
ylab = "True Positive Rate (Sensitivity)")
plot(roc_curve1, col = pastel_red, add = TRUE, lwd = 2)
# Add legend with pastel colors and labels
legend("bottomright", legend = c("Naive Bayes", "Decision Trees"), col = c(pastel_blue, pastel_red), lty = 1, lwd = 2, cex = 0.8)
# Add grid lines for better readability
grid()
# Add a smooth line through the ROC curve
smooth_ROC_curve <- smooth(roc_curve, method = "spline")
# Define pastel colors
pastel_blue <- "#ADD8E6"  # Light blue
pastel_red <- "#FFC0CB"   # Light pink
# Plot ROC curves with pastel colors
plot(roc_curve, col = pastel_blue, main = "ROC Curves for Naive Bayes and Decision Trees",
xlim = c(0, 1), ylim = c(0, 1), type = "l", lwd = 2, xlab = "False Positive Rate (1 - Specificity)",
ylab = "True Positive Rate (Sensitivity)")
plot(roc_curve1, col = pastel_red, add = TRUE, lwd = 2)
# Add legend with pastel colors and labels
legend("bottomright", legend = c("Naive Bayes", "Decision Trees"), col = c(pastel_blue, pastel_red), lty = 1, lwd = 2, cex = 0.8)
# Add grid lines for better readability
grid()
# Add a smooth line through the ROC curve
smooth_ROC_curve <- smooth(roc_curve, method = "spline")
plot(roc_curve1, col = "#ADD8E6", main = "ROC Curves for Naive Bayes and Decision Trees")
plot(roc_curve, col = "#FFC0CB", add = TRUE)
# Add legend
legend("bottomright", legend = c("Naive Bayes", "Decision Trees"), col = c("blue", "red"), lty = 1, cex = 0.8)
grid()
plot(roc_curve1, col = "#ADD8E6", main = "ROC Curves for Naive Bayes and Decision Trees")
plot(roc_curve, col = "#FFC0CB", add = TRUE)
# Add legend
legend("bottomright", legend = c("Naive Bayes", "Decision Trees"), col = c("blue", "red"), lty = 1, cex = 0.8)
grid()
